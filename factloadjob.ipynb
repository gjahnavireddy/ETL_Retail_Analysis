{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Visualize data with matplotlib\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import matplotlib.pyplot as plt\n\n# Set X-axis and Y-axis values\nx = [5, 2, 8, 4, 9]\ny = [10, 4, 8, 5, 2]\n  \n# Create a bar chart \nplt.bar(x, y)\n  \n# Show the plot\n%matplot plt",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport boto3\nimport pandas as pd\n\n# Input arguments for the Glue job\nargs = sys.argv\nsource_bucket_name = \"team11projectdw\"  # Source S3 bucket for dimension tables\ntarget_bucket_name = \"team11projectdw\"  # Target S3 bucket for SalesFact table\nolap_prefix = \"olap/\"  # Folder for transformed data\n\n# File paths for existing dimension tables\norder_dimension_file = f\"s3://{source_bucket_name}/{olap_prefix}order_dimension/order_dimension.csv\"\nreview_dimension_file = f\"s3://{source_bucket_name}/{olap_prefix}review_dimension/review_dimension.csv\"\nproduct_dimension_file = f\"s3://{source_bucket_name}/{olap_prefix}product_dimension/product_dimension.csv\"\nseller_dimension_file = f\"s3://{source_bucket_name}/{olap_prefix}seller_dimension/seller_dimension.csv\"\n\n# --- Step 1: Load the dimension data into Pandas DataFrames ---\nprint(\"Loading dimension tables from S3...\")\ndf_order_dimension = pd.read_csv(order_dimension_file)\ndf_review_dimension = pd.read_csv(review_dimension_file)\ndf_product_dimension = pd.read_csv(product_dimension_file)\ndf_seller_dimension = pd.read_csv(seller_dimension_file)\n\n# --- Step 2: Create SalesFact Table ---\nprint(\"Creating SalesFact table...\")\n\n# Merge dimensions\nsalesfact = df_order_dimension.merge(df_review_dimension, on='order_id', how='left') \\\n                               .merge(df_product_dimension, on='product_id', how='left') \\\n                               .merge(df_seller_dimension, on='seller_id', how='left')\n\n# Calculate derived columns\nsalesfact['productvolume'] = (salesfact['product_length'] * \n                              salesfact['product_width'] * \n                              salesfact['product_height'])\nsalesfact['totalordervalue'] = salesfact['price'] + salesfact['freight_value']\n\n# Select relevant columns for the SalesFact table\nsalesfact = salesfact[['Orderkey', 'Reviewkey', 'Productkey', 'Sellerkey', \n                      'price', 'freight_value', 'product_weight', 'product_length', \n                      'product_width', 'product_height', 'productvolume', \n                      'totalordervalue', 'score']]  # Adding reviewscore as 'score'\n\n# Rename the column 'score' to 'reviewscore' for better clarity\nsalesfact.rename(columns={'score': 'reviewscore'}, inplace=True)\n\n# --- Step 3: Check for duplicates and drop them ---\nprint(\"Checking for duplicates in SalesFact table...\")\nsalesfact = salesfact.drop_duplicates().reset_index(drop=True)\n\n# --- Step 4: Save Transformed SalesFact Data Locally ---\nprint(\"Saving SalesFact data locally...\")\nsalesfact.to_csv(\"/tmp/salesfact.csv\", index=False)\n\n# --- Step 5: Upload SalesFact Data to S3 ---\nprint(\"Uploading SalesFact data to S3...\")\ns3 = boto3.client('s3')\ns3.upload_file(\"/tmp/salesfact.csv\", target_bucket_name, f\"{olap_prefix}salesfact/salesfact.csv\")\n\nprint(\"SalesFact table created and uploaded successfully!\")\n",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}