{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Visualize data with matplotlib\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import matplotlib.pyplot as plt\n\n# Set X-axis and Y-axis values\nx = [5, 2, 8, 4, 9]\ny = [10, 4, 8, 5, 2]\n  \n# Create a bar chart \nplt.bar(x, y)\n  \n# Show the plot\n%matplot plt",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport boto3\nimport pandas as pd\n\n# Input arguments for the Glue job\nargs = sys.argv\nsource_bucket_name = \"team11project\"  # Source S3 bucket\ntarget_bucket_name = \"team11projectdw\"  # Target S3 bucket\noltp_prefix = \"oltp/\"  # Folder containing source data\nolap_prefix = \"olap/\"  # Folder for transformed data in the target bucket\n\n# File paths for source data\norder_items_file = f\"s3://{source_bucket_name}/{oltp_prefix}order.csv\"\norder_reviews_file = f\"s3://{source_bucket_name}/{oltp_prefix}order_reviews.csv\"\nproducts_file = f\"s3://{source_bucket_name}/{oltp_prefix}products.csv\"\nsellers_file = f\"s3://{source_bucket_name}/{oltp_prefix}sellers.parquet\"\n\n# --- Step 1: Load the data into Pandas DataFrames ---\nprint(\"Loading data from S3...\")\ndf_order_items = pd.read_csv(order_items_file)\ndf_order_reviews = pd.read_csv(order_reviews_file)\ndf_products = pd.read_csv(products_file)\ndf_sellers = pd.read_parquet(sellers_file)\n\n# --- Step 2: Create Order Dimension ---\nprint(\"Creating Order Dimension...\")\norder_dimension = df_order_items.copy()\n\n# Add 'Orderkey' as a unique identifier\norder_dimension['Orderkey'] = range(1, len(order_dimension) + 1)\n\n# Select relevant columns, including 'shipping_limit_date'\norder_dimension = order_dimension[['Orderkey', 'order_id', 'product_id', 'seller_id', 'price', 'freight_value', 'shipping_limit_date']]\n\n# Check for duplicates and drop them\nprint(\"Checking for duplicates in Order Dimension...\")\norder_dimension = order_dimension.drop_duplicates().reset_index(drop=True)\n\n# --- Step 3: Create Review Dimension ---\nprint(\"Creating Review Dimension...\")\nreview_dimension = df_order_reviews.copy()\nreview_dimension['Reviewkey'] = range(1, len(review_dimension) + 1)\nreview_dimension = review_dimension[['Reviewkey', 'order_id', 'review_score']].rename(columns={'review_score': 'score'})\n\n# Check for duplicates and drop them\nprint(\"Checking for duplicates in Review Dimension...\")\nreview_dimension = review_dimension.drop_duplicates().reset_index(drop=True)\n\n# --- Step 4: Create Product Dimension ---\nprint(\"Creating Product Dimension...\")\nproduct_dimension = df_products.copy()\nproduct_dimension['Productkey'] = range(1, len(product_dimension) + 1)\nproduct_dimension = product_dimension[['Productkey', 'product_id', 'product_category_name', \n                                       'product_weight_g', 'product_length_cm', \n                                       'product_height_cm', 'product_width_cm']].rename(columns={\n    'product_weight_g': 'product_weight',\n    'product_length_cm': 'product_length',\n    'product_height_cm': 'product_height',\n    'product_width_cm': 'product_width'\n})\n\n# Check for duplicates and drop them\nprint(\"Checking for duplicates in Product Dimension...\")\nproduct_dimension = product_dimension.drop_duplicates().reset_index(drop=True)\n\n# --- Step 5: Create Seller Dimension ---\nprint(\"Creating Seller Dimension...\")\nseller_dimension = df_sellers.copy()\nseller_dimension['Sellerkey'] = range(1, len(seller_dimension) + 1)\nseller_dimension = seller_dimension[['Sellerkey', 'seller_id', 'seller_zip_code_prefix', \n                                     'seller_city', 'seller_state']].rename(columns={\n    'seller_zip_code_prefix': 'seller_zipcode'\n})\n\n# Check for duplicates and drop them\nprint(\"Checking for duplicates in Seller Dimension...\")\nseller_dimension = seller_dimension.drop_duplicates().reset_index(drop=True)\n\n# --- Step 6: Save Transformed Data Locally ---\nprint(\"Saving transformed data locally...\")\norder_dimension.to_csv(\"/tmp/order_dimension.csv\", index=False)\nreview_dimension.to_csv(\"/tmp/review_dimension.csv\", index=False)\nproduct_dimension.to_csv(\"/tmp/product_dimension.csv\", index=False)\nseller_dimension.to_csv(\"/tmp/seller_dimension.csv\", index=False)\n\n# --- Step 7: Upload Transformed Data to the New Target S3 Bucket ---\nprint(\"Uploading transformed data to the target S3 bucket...\")\ns3 = boto3.client('s3')\n\n# Upload each dimension to the appropriate path in the target bucket\ns3.upload_file(\"/tmp/order_dimension.csv\", target_bucket_name, f\"{olap_prefix}order_dimension/order_dimension.csv\")\ns3.upload_file(\"/tmp/review_dimension.csv\", target_bucket_name, f\"{olap_prefix}review_dimension/review_dimension.csv\")\ns3.upload_file(\"/tmp/product_dimension.csv\", target_bucket_name, f\"{olap_prefix}product_dimension/product_dimension.csv\")\ns3.upload_file(\"/tmp/seller_dimension.csv\", target_bucket_name, f\"{olap_prefix}seller_dimension/seller_dimension.csv\")\n\nprint(\"Transformation and upload to the target bucket complete!\")\n",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}